{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "brQEAK7B0wEH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c920d98f-d096-453f-a07a-f60134aff33f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_tuner\n",
            "  Downloading keras_tuner-1.1.3-py3-none-any.whl (135 kB)\n",
            "\u001b[K     |████████████████████████████████| 135 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (21.3)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (2.23.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (7.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (1.21.6)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras_tuner) (2.8.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 33.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (2.0.10)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (5.1.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras_tuner) (57.4.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->keras_tuner) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras_tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras_tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras_tuner) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras_tuner) (0.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras_tuner) (2.10)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (1.47.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras_tuner) (3.17.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras_tuner) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras_tuner) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras_tuner) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras_tuner) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras_tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras_tuner) (3.2.0)\n",
            "Installing collected packages: jedi, kt-legacy, keras-tuner\n",
            "Successfully installed jedi-0.18.1 keras-tuner-1.1.3 kt-legacy-1.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install -U keras_tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPypSw8CAFHN"
      },
      "source": [
        "Keras Tuner is an easy-to-use, distributable hyperparameter optimization framework that solves the pain points of performing a hyperparameter search. Keras Tuner makes it easy to define a search space and leverage included algorithms to find the best hyperparameter values. Keras Tuner comes with Bayesian Optimization, Hyperband, and Random Search algorithms built-in, and is also designed to be easy for researchers to extend in order to experiment with new search algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71oVAc480vKk",
        "outputId": "254f7a62-1da3-4b99-b9c7-e2a55644156e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras Tuner Version : 1.1.3\n"
          ]
        }
      ],
      "source": [
        "import keras_tuner\n",
        "\n",
        "print(\"Keras Tuner Version : {}\".format(keras_tuner.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7TQ9-9306eV",
        "outputId": "476f11d4-1f6a-4a20-9ca1-e67e06b57430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras Version : 2.8.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "print(\"Keras Version : {}\".format(keras.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRDD-dn73Cvl"
      },
      "source": [
        "# 2. Classification Example (Random Hyperparameters Search) \n",
        "\n",
        "As a part of project, we will use a random search tuner for classification tasks. We have loaded the Fashion MNIST dataset below for our task. The dataset has grayscale images of shape (28,28) pixels for 10 different fashion items. \n",
        "\n",
        "The dataset is already divided into the train (60k images) and test (10k images) sets. We'll be trying various convolutional neural networks on this dataset to check which one is giving the best results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1Jg54GG22S_",
        "outputId": "998cc7ec-9394-40bd-fcd1-5de38e3eea6b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28, 1), (10000, 28, 28, 1), (60000,), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import datasets\n",
        "\n",
        "(X_train_classif, Y_train_classif), (X_test_classif, Y_test_classif) = datasets.fashion_mnist.load_data()\n",
        "\n",
        "X_train_classif, X_test_classif = X_train_classif.reshape(-1,28,28,1), X_test_classif.reshape(-1,28,28,1)\n",
        "\n",
        "classes = np.unique(Y_train_classif)\n",
        "\n",
        "X_train_classif.shape, X_test_classif.shape, Y_train_classif.shape, Y_test_classif.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dewAvdin3Mkc"
      },
      "source": [
        "# Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamzG8F1AFHR"
      },
      "source": [
        "In the below cell, we have created a new class that extends HyperModel class. The class has build() method that takes HyperParameters instance as input and returns a compiled keras model. It sets various hyperparameters using methods of HyperParameters instance. We'll be giving an instance of this class to RandomSearch() constructor later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_PNEswiJ3GHW"
      },
      "outputs": [],
      "source": [
        "from keras_tuner import HyperModel\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "class ConvNetwork(HyperModel):\n",
        "    def build(self, hp):\n",
        "        model = Sequential()\n",
        "        model.add(layers.Input(shape=X_train_classif.shape[1:]))\n",
        "        model_type = hp.Choice(\"ConvNetType\", [\"Conv1\",\"Conv2\"])\n",
        "\n",
        "        if model_type == \"Conv1\":\n",
        "            with hp.conditional_scope(\"ConvNetType\", [\"Conv1\"]):\n",
        "                activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
        "                kern_init = hp.Choice(\"kernel_initializer\", [\"random_normal\", \"lecun_normal\",\"he_normal\"])\n",
        "\n",
        "                model.add(layers.Conv2D(filters=hp.Int(\"Conv1_1\", 16, 33, step=16), kernel_size=(3,3), padding=\"same\", kernel_initializer=kern_init, activation=activation))\n",
        "                model.add(layers.Conv2D(filters=hp.Int(\"Conv1_2\", 16, 33, step=16), kernel_size=(3,3), padding=\"same\", kernel_initializer=kern_init, activation=activation))\n",
        "        elif model_type == \"Conv2\":\n",
        "            with hp.conditional_scope(\"ConvNetType\", [\"Conv2\"]):\n",
        "                activation = hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
        "                kern_init = hp.Choice(\"kernel_initializer\", [\"random_normal\", \"lecun_normal\",\"he_normal\"])\n",
        "\n",
        "                model.add(layers.Conv2D(filters=hp.Int(\"Conv2_1\", 16, 33, step=16), kernel_size=(3,3), padding=\"same\", kernel_initializer=kern_init, activation=activation))\n",
        "                model.add(layers.Conv2D(filters=hp.Int(\"Conv2_2\", 16, 33, step=16), kernel_size=(3,3), padding=\"same\", kernel_initializer=kern_init, activation=activation))\n",
        "                model.add(layers.Conv2D(filters=hp.Int(\"Conv2_3\", 8, 17, step=8), kernel_size=(3,3), padding=\"same\", kernel_initializer=kern_init, activation=activation))\n",
        "\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(units=len(classes), activation=\"softmax\"))\n",
        "\n",
        "        model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmh-ceHy3XMp"
      },
      "source": [
        "# RandomSearch\n",
        "\n",
        "In the below cell, we have created a random search tuner and executed it for 5 trials. We have given our instance of HyperModel to it and have asked it to maximize validation accuracy using Objective instance.\n",
        "\n",
        "We have executed the tuning process by calling search() function giving it train data validation data, batch size (512), and epochs (10).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq5s8Mo83OxY",
        "outputId": "a84b0fed-9c53-4cdd-831f-1df1fcdc27d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1 Complete [00h 01m 32s]\n",
            "val_accuracy: 0.2870999872684479\n",
            "\n",
            "Best val_accuracy So Far: 0.2870999872684479\n",
            "Total elapsed time: 00h 01m 32s\n"
          ]
        }
      ],
      "source": [
        "from keras_tuner import RandomSearch\n",
        "from keras_tuner import Objective\n",
        "\n",
        "conv2 = ConvNetwork()\n",
        "tuner2 =  RandomSearch(hypermodel=conv2,\n",
        "                      objective=Objective(name=\"val_accuracy\",direction=\"max\"),\n",
        "                      max_trials=1,\n",
        "                      #seed=123,\n",
        "                      project_name=\"Classification\",\n",
        "                      overwrite=True\n",
        "                    )\n",
        "\n",
        "tuner2.search(X_train_classif, Y_train_classif, batch_size=512812, epochs=1, validation_data=(X_test_classif, Y_test_classif))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4puMGhp5AFHU"
      },
      "source": [
        "In the next cells, we have retrieved the best model and used it to evaluate performance on the test dataset which we had used as a validation dataset. Then, we have printed the tuning summary as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GSiyRBhU3Z9s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3f1218-d18c-4535-8bf6-a62c8f59745c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ConvNetType': 'Conv2',\n",
              " 'activation': 'relu',\n",
              " 'kernel_initializer': 'random_normal',\n",
              " 'Conv2_1': 16,\n",
              " 'Conv2_2': 16,\n",
              " 'Conv2_3': 8}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "best_params = tuner2.get_best_hyperparameters()\n",
        "\n",
        "best_params[0].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JdcfjyC03wx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7b0243-e50c-453b-9724-7133935e6ada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 16)        160       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 28, 28, 16)        2320      \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 28, 28, 8)         1160      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                62730     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66,370\n",
            "Trainable params: 66,370\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "best_model = tuner2.get_best_models()[0]\n",
        "\n",
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1mW0JoyL4RW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb8d14da-501a-421e-d9c8-a541eb7ab885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 35s 110ms/step - loss: 1.9415 - accuracy: 0.2871\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.9415361881256104, 0.2870999872684479]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "best_model.evaluate(X_test_classif, Y_test_classif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5322jJBy33U2"
      },
      "source": [
        "# Hyperband Algorithm\n",
        "\n",
        "In this section, we have performed hyperparameters optimization using Hyperband algorithm. It is a variation of random search with explore-exploit theory to find good hyperparameters settings. It focuses on speeding up random search through adaptive resource allocation and early stopping. \n",
        "\n",
        "It randomly allocates resources like iterations, data samples, and features to different hyperparameters settings and tries to solve stochastic bandit problems where it keeps on eliminating underperforming settings. The keras tuner provides an implementation of Hyperband algorithm tuner through Hyperband() constructor. \n",
        "\n",
        "It has the majority of the parameters same as random search with a few additional parameters as listed below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gZS5w7oF34qy"
      },
      "outputs": [],
      "source": [
        "from keras_tuner import Hyperband\n",
        "from keras_tuner import Objective"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSHOhiLL37kJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e18311-ef1b-4f74-91fe-32ee5c2f24a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 13 Complete [00h 05m 23s]\n",
            "val_accuracy: 0.8873000144958496\n",
            "\n",
            "Best val_accuracy So Far: 0.8985000252723694\n",
            "Total elapsed time: 00h 59m 53s\n",
            "\n",
            "Search: Running Trial #14\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "Conv2             |Conv2             |ConvNetType\n",
            "tanh              |tanh              |activation\n",
            "he_normal         |random_normal     |kernel_initializer\n",
            "32                |16                |Conv2_1\n",
            "32                |16                |Conv2_2\n",
            "16                |8                 |Conv2_3\n",
            "2                 |2                 |tuner/epochs\n",
            "0                 |0                 |tuner/initial_epoch\n",
            "4                 |4                 |tuner/bracket\n",
            "0                 |0                 |tuner/round\n",
            "\n",
            "Epoch 1/2\n",
            "469/469 [==============================] - 214s 456ms/step - loss: 0.3921 - accuracy: 0.8641 - val_loss: 0.3466 - val_accuracy: 0.8841\n",
            "Epoch 2/2\n",
            "272/469 [================>.............] - ETA: 1:26 - loss: 0.2321 - accuracy: 0.9176"
          ]
        }
      ],
      "source": [
        "conv6 = ConvNetwork()\n",
        "tuner6 =  Hyperband(hypermodel=conv6,\n",
        "                   objective=Objective(name=\"val_accuracy\",direction=\"max\"),\n",
        "                   hyperband_iterations=1,\n",
        "                   #seed=123\n",
        "                   project_name=\"Hyperband\",\n",
        "                   overwrite=True\n",
        "                  )\n",
        "\n",
        "tuner6.search(X_train_classif, Y_train_classif, batch_size=128, epochs=1, validation_data=(X_test_classif, Y_test_classif))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkeGR0-I4F2j"
      },
      "outputs": [],
      "source": [
        "best_params = tuner6.get_best_hyperparameters()\n",
        "\n",
        "best_params[0].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNghGpGG4GR-"
      },
      "outputs": [],
      "source": [
        "best_model = tuner6.get_best_models()[0]\n",
        "\n",
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riHIK9p44LeL"
      },
      "outputs": [],
      "source": [
        "best_model.evaluate(X_test_classif, Y_test_classif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE5o6xrP4XZP"
      },
      "source": [
        "# Bayesian Optimization Algorithm \n",
        "\n",
        "In this example, I have already explained bayesian optimization tuner available from keras tuner. Bayesian optimization uses Bayes theorem to find the best hyperparameters settings. We can use the Bayesian optimization tuner by BayesianOptimization() constructor of the keras tuner. It has almost the same parameters as a random search tuner with a few additional parameters listed below.\n",
        "\n",
        "num_initial_points - This parameter accepts integer values specifying the number of randomly generated samples for the initial training of the network. The default is 2.\n",
        "\n",
        "alpha - This parameter accepts float value added to the diagonal of kernel matrix during fitting. It is the expected amount of noise in the observed performances in the Bayesian optimization process. The default value is 1e-4.\n",
        "\n",
        "beta - This parameter accepts float value specifying balancing factor of exploration and exploitation. The larger value means more exploration. The default value is 2.6.\n",
        "\n",
        "Below, we have initialized the bayesian optimization tuner and tried to find good hyperparameters settings for our classification task network (CNN). As usual, we have performed a search by calling search() method on the tuner object.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHlgZRi_4hkg"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from keras_tuner import BayesianOptimization\n",
        "from keras_tuner import Objective\n",
        "\n",
        "conv7 = ConvNetwork()\n",
        "tuner7 =  BayesianOptimization(hypermodel=conv7,\n",
        "                               objective=Objective(name=\"val_accuracy\",direction=\"max\"),\n",
        "                               max_trials=10,\n",
        "                               num_initial_points=2,\n",
        "                               #seed=123\n",
        "                               project_name=\"BayesianOptimization\",\n",
        "                               overwrite=True\n",
        "                              )\n",
        "\n",
        "tuner7.search(X_train_classif, Y_train_classif, batch_size=128, epochs=5, validation_data=(X_test_classif, Y_test_classif))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm4mYtVq4jUD"
      },
      "outputs": [],
      "source": [
        "best_params = tuner7.get_best_hyperparameters()\n",
        "\n",
        "best_params[0].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_VopB-m4mjh"
      },
      "outputs": [],
      "source": [
        "best_model = tuner7.get_best_models()[0]\n",
        "\n",
        "best_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivlIHke74o_Z"
      },
      "outputs": [],
      "source": [
        "best_model.evaluate(X_test_classif, Y_test_classif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCCJZ9smAFHY"
      },
      "source": [
        "This ends our small tutorial explaining how we can use various tuners available from keras tuner to find the best hyperparameters for the given model. I have explained all hyperparameters tuning algorithms available from keras tuner. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exD6WQOVAFHY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}